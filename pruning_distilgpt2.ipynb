{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "z8x0-Xvb8_R9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QPsozhex5ng4"
      },
      "outputs": [],
      "source": [
        "# required libraries\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from transformers.models.gpt2.modeling_gpt2 import Conv1D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMpbJPg06F2v",
        "outputId": "7a0ba9c6-2f04-401f-fb1a-f7ea669b7f77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Loading"
      ],
      "metadata": {
        "id": "ye3I03Eu9Dwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model\n",
        "model_name = \"distilgpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSnTY_166cEv",
        "outputId": "5be4c21f-6660-47cd-be52-3afae4b1ea7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support functions"
      ],
      "metadata": {
        "id": "YYXUXSJC9KXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to check the size of the model\n",
        "def count_params(model):\n",
        "  return sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model name: {model_name}, Parameters count: {count_params(model)} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjcwBFtb7bAo",
        "outputId": "0349c570-0fe9-4c92-deb5-be0a94c2ca29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name: distilgpt2, Parameters count: 81912576 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output(prompt,model,tokenizer):\n",
        "  inputs = tokenizer(prompt,return_tensors='pt').to(device)\n",
        "  outputs = model.generate(inputs['input_ids'],attention_mask=inputs['attention_mask'],max_length=10,num_return_sequences=1)\n",
        "  generated = tokenizer.batch_decode(outputs[0],skip_special_tokens=True)\n",
        "  return generated"
      ],
      "metadata": {
        "id": "npZ6sNCT8INX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model summary"
      ],
      "metadata": {
        "id": "VEeHxW9b9fBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRA2MGoz8-yS",
        "outputId": "dbeb6b75-533c-4beb-81f4-262629fde1a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-5): 6 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loWGMgsQ9VF3",
        "outputId": "7c907b9b-28ba-48fc-8422-858779531814"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.48.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the original model\n",
        "prompt = \"Paris is the capital of\"\n",
        "generated =get_output(prompt,model,tokenizer)\n",
        "print(f\"Base Models generated response: {generated}\")\n",
        "base_params_count = count_params(model)\n",
        "print(f\"Base Models parameters: {base_params_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WgS5DtH9iZi",
        "outputId": "f3b6dda3-482e-475a-e071-ee86463f5e35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Models generated response: ['Paris', ' is', ' the', ' capital', ' of', ' the', ' United', ' States', '.', '\\n']\n",
            "Base Models parameters: 81912576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning the model"
      ],
      "metadata": {
        "id": "cFo_hqfd-d_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize new_intermediate_size\n",
        "new_intermediate_size = None"
      ],
      "metadata": {
        "id": "pB_M9ldv9xwD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pruning support functions"
      ],
      "metadata": {
        "id": "Ts62e2Vj-8Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for computing important scores based on L1 norm\n",
        "def compute_imp_score(c_fc_weight):\n",
        "  return torch.sum(torch.abs(c_fc_weight), dim=0)  # Shape: [intermediate_size]"
      ],
      "metadata": {
        "id": "lQwoW3G5-8I2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prune neurons and create new Conv1D layers\n",
        "def prune_neurons(mlp, prune_percent, device):\n",
        "    # Get the weights of the c_fc layer (input projection)\n",
        "    c_fc_weight = mlp.c_fc.weight.data\n",
        "\n",
        "    # Compute importance scores for each neuron\n",
        "    importance_scores = compute_imp_score(c_fc_weight)\n",
        "\n",
        "    # Determine the number of neurons to prune\n",
        "    original_intermediate_size = c_fc_weight.size(1)  # This is intermediate_size\n",
        "    num_neurons_to_prune = int(prune_percent * original_intermediate_size)\n",
        "\n",
        "    # Get indices of neurons to keep (those with highest importance)\n",
        "    _, indices_to_keep = torch.topk(importance_scores, original_intermediate_size - num_neurons_to_prune)\n",
        "\n",
        "    # Sort indices to maintain order\n",
        "    indices_to_keep, _ = torch.sort(indices_to_keep)\n",
        "\n",
        "    # Create new Conv1D layers with reduced size\n",
        "    new_c_fc = Conv1D(len(indices_to_keep), mlp.c_fc.weight.size(0)).to(device)  # Conv1D(new_intermediate_size, hidden_size)\n",
        "    new_c_proj = Conv1D(mlp.c_proj.weight.size(1), len(indices_to_keep)).to(device)  # Conv1D(hidden_size, new_intermediate_size)\n",
        "\n",
        "    return new_c_fc, new_c_proj, len(indices_to_keep), indices_to_keep"
      ],
      "metadata": {
        "id": "UVXtXoKA-6in"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to copy weights and biases to new pruned layers\n",
        "def copy_weights_and_biases(mlp, new_c_fc, new_c_proj, indices_to_keep):\n",
        "    # Copy weights and biases for the neurons we are keeping and move them to the specified device\n",
        "    new_c_fc.weight.data = mlp.c_fc.weight.data[:, indices_to_keep].to(device)\n",
        "    new_c_fc.bias.data = mlp.c_fc.bias.data[indices_to_keep].to(device)\n",
        "\n",
        "    new_c_proj.weight.data = mlp.c_proj.weight.data[indices_to_keep, :].to(device)\n",
        "    new_c_proj.bias.data = mlp.c_proj.bias.data.to(device)"
      ],
      "metadata": {
        "id": "odu5nMoQDT-z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pruning loop"
      ],
      "metadata": {
        "id": "WKOBznSQBshk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_model(model, prune_percent, device):\n",
        "    new_intermediate_size = None\n",
        "\n",
        "    # Iterate through each block in the model\n",
        "    for idx, block in enumerate(model.transformer.h):\n",
        "        mlp = block.mlp\n",
        "\n",
        "        # Prune the neurons and create new layers\n",
        "        new_c_fc, new_c_proj, new_size, indices_to_keep = prune_neurons(mlp, prune_percent, device)\n",
        "\n",
        "        # Copy weights and biases from old layers to new pruned layers\n",
        "        copy_weights_and_biases(mlp, new_c_fc, new_c_proj, indices_to_keep)\n",
        "\n",
        "        # Replace old layers with new pruned layers\n",
        "        mlp.c_fc = new_c_fc\n",
        "        mlp.c_proj = new_c_proj\n",
        "\n",
        "        # Update the intermediate size for the first block\n",
        "        if new_intermediate_size is None:\n",
        "            new_intermediate_size = new_size\n",
        "\n",
        "    # Update the model configuration with the new intermediate size\n",
        "    model.config.n_inner = new_intermediate_size\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9aNOc8k2Bex2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the pruned model"
      ],
      "metadata": {
        "id": "b_VwWkaKCHl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prune percentage\n",
        "prune_percent = 0.3"
      ],
      "metadata": {
        "id": "0hIt3BfYCQq0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = update_model(model,prune_percent,device)"
      ],
      "metadata": {
        "id": "oHHb_tY7CFtn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the parameter count\n",
        "pruned_params_count = count_params(model)\n",
        "print(f\"Base Models parameters count: {base_params_count}\")\n",
        "print(f\"Prune Models parameters count: {pruned_params_count}\")\n",
        "print(f\"Reduction in parameters: {base_params_count - pruned_params_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en2WY8vBCVDx",
        "outputId": "4bfa2846-4767-4120-e7fd-2c3e2701ca0c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Models parameters count: 81912576\n",
            "Prune Models parameters count: 73419114\n",
            "Reduction in parameters: 8493462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fariq4oRCmV9",
        "outputId": "061b0bc9-7f31-45a7-997e-f28bad3f4c7c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-5): 6 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=2151, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=2151)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#config file pruned model.\n",
        "print(model.config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4qP9pyWECge",
        "outputId": "b04c390a-d94a-4f11-c908-f86a066eda63"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": 2151,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.48.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking again\n",
        "generated = get_output(prompt,model,tokenizer)\n",
        "print(f\"Generated text after pruning: {generated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCzS2NaoEJs5",
        "outputId": "bc6e2945-29d9-4b79-c9c3-dbce1ed83405"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text after pruning: ['Paris', ' is', ' the', ' capital', ' of', ' the', ' United', ' States', '.', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "5qWREDuiFgN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the pruned model\n",
        "output_dir = './pruned_distilgpt2'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Pruned model saved to {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra_CrOWNEPpb",
        "outputId": "3ecc6d57-338f-45a3-ec6b-45ca42ab1f1d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned model saved to ./pruned_distilgpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kaQafScMFk4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}