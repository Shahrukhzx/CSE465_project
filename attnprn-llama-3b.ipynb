{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5d4b0da233e144258fdf0d6e802af69a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5269522a9c844b00aa65d50d817d0973","IPY_MODEL_070a662cb4494191be279127f53a6f9b","IPY_MODEL_b9e7043d1bf042c38473e9036bfa9d79"],"layout":"IPY_MODEL_7342a8e859a64c81bd8c85ca9e5b0141"}},"5269522a9c844b00aa65d50d817d0973":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_086ba5e6e4b442dfa79a74e0bbb94b32","placeholder":"​","style":"IPY_MODEL_c4f3670092ad47179a9947a606990c08","value":"Loading checkpoint shards:  50%"}},"070a662cb4494191be279127f53a6f9b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf1bd8403ec144009394942aa408a710","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a67a7331ef384d31aea5ba7ff418080a","value":1}},"b9e7043d1bf042c38473e9036bfa9d79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8e17d3dcae54d568ad99f8d3212b23b","placeholder":"​","style":"IPY_MODEL_99be618c1b6845ceb4764477c476e8d2","value":" 1/2 [00:21&lt;00:21, 21.42s/it]"}},"7342a8e859a64c81bd8c85ca9e5b0141":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"086ba5e6e4b442dfa79a74e0bbb94b32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4f3670092ad47179a9947a606990c08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf1bd8403ec144009394942aa408a710":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a67a7331ef384d31aea5ba7ff418080a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8e17d3dcae54d568ad99f8d3212b23b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99be618c1b6845ceb4764477c476e8d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"PPTjx4Vq_5Zo"}},{"cell_type":"code","source":"#!pip install -q transformers==4.47.1\n#!pip install -q datasets==3.2.0\n#!pip install -q torch==2.5.1\n#!pip install -q lm-eval==0.4.7","metadata":{"id":"S3Egs5kZ-eIl","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:26.844561Z","iopub.execute_input":"2025-02-14T22:41:26.844882Z","iopub.status.idle":"2025-02-14T22:41:26.848427Z","shell.execute_reply.started":"2025-02-14T22:41:26.844861Z","shell.execute_reply":"2025-02-14T22:41:26.847664Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import logging\nimport math\nimport os\nimport sys\nimport shutil\nfrom copy import deepcopy\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport os","metadata":{"id":"7m7gGPkiAAci","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:26.855005Z","iopub.execute_input":"2025-02-14T22:41:26.855243Z","iopub.status.idle":"2025-02-14T22:41:30.396628Z","shell.execute_reply.started":"2025-02-14T22:41:26.855223Z","shell.execute_reply":"2025-02-14T22:41:30.395717Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Model Loading\n","metadata":{"id":"qOgu7npPAYW4"}},{"cell_type":"code","source":"# device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device: {device}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FRHyAYMlAcFz","outputId":"4248165c-8f46-4463-a41b-92365b858c10","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:30.397560Z","iopub.execute_input":"2025-02-14T22:41:30.397933Z","iopub.status.idle":"2025-02-14T22:41:30.424732Z","shell.execute_reply.started":"2025-02-14T22:41:30.397911Z","shell.execute_reply":"2025-02-14T22:41:30.423863Z"}},"outputs":[{"name":"stdout","text":"Using device: {device}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.cuda.memory_summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:30.425756Z","iopub.execute_input":"2025-02-14T22:41:30.426107Z","iopub.status.idle":"2025-02-14T22:41:30.463103Z","shell.execute_reply.started":"2025-02-14T22:41:30.426076Z","shell.execute_reply":"2025-02-14T22:41:30.462430Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model_name = 'meta-llama/Llama-3.2-3B'\nmodel = AutoModelForCausalLM.from_pretrained(model_name,offload_buffers=True, torch_dtype=torch.float16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153,"referenced_widgets":["5d4b0da233e144258fdf0d6e802af69a","5269522a9c844b00aa65d50d817d0973","070a662cb4494191be279127f53a6f9b","b9e7043d1bf042c38473e9036bfa9d79","7342a8e859a64c81bd8c85ca9e5b0141","086ba5e6e4b442dfa79a74e0bbb94b32","c4f3670092ad47179a9947a606990c08","bf1bd8403ec144009394942aa408a710","a67a7331ef384d31aea5ba7ff418080a","a8e17d3dcae54d568ad99f8d3212b23b","99be618c1b6845ceb4764477c476e8d2"]},"id":"JCy3zNeJAXen","outputId":"e3b94af5-8758-4f8c-d99f-610a6484f896","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:30.468231Z","iopub.execute_input":"2025-02-14T22:41:30.468518Z","iopub.status.idle":"2025-02-14T22:41:41.261162Z","shell.execute_reply.started":"2025-02-14T22:41:30.468488Z","shell.execute_reply":"2025-02-14T22:41:41.260339Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"241e4b0197de42ec82cb4d329e96dace"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# structure of the model\nprint(model)","metadata":{"id":"sRSAX1DxBCgB","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:41.263999Z","iopub.execute_input":"2025-02-14T22:41:41.264529Z","iopub.status.idle":"2025-02-14T22:41:41.270585Z","shell.execute_reply.started":"2025-02-14T22:41:41.264505Z","shell.execute_reply":"2025-02-14T22:41:41.269825Z"}},"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 3072)\n    (layers): ModuleList(\n      (0-27): 28 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Prunning\n","metadata":{"id":"mxIxvuK2BQuC"}},{"cell_type":"markdown","source":"## Support functions","metadata":{"id":"Uqqr4v8SBhDh"}},{"cell_type":"code","source":"def measure_unpruned_layer_importances(pruned_model, tokenizer, input_text):\n    \"\"\"\n    Measures and returns importance scores for all unpruned (non-bypassed) layers.\n    \"\"\"\n    # PREPARATION\n    \"\"\"\n    set the model to evaluation mode to ensure that no gradients\n    are computed during the forward pass.\n    \"\"\"\n    pruned_model.eval()\n    device = next(pruned_model.parameters()).device\n\n    \"\"\"\n    The provided input text (input_text) is tokenized into tensors\n    suitable for processing by the model.\n    \"\"\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n\n    \"\"\"This will hold tuples of (layer_idx, importance_score)\"\"\"\n    importance_scores = []\n\n    # IDENTIFY UNPRUNED LAYERS & CREATING HOOKS\n    \"\"\"\n    We'll register hooks for only layers that are NOT in drop_attn_list\n    The list of attention layers that have already been pruned,\n    is stored in a variable in the model's config: pruned_model.config.drop_attn_list.\n    \"\"\"\n    unpruned_layer_indices = [\n        idx for idx in range(len(pruned_model.model.layers))\n        if idx not in pruned_model.config.drop_attn_list\n    ]\n\n    \"\"\"\n    Temporary storage for each layer's input/output\n    We'll store them by layer index\n    \"\"\"\n    layer_inputs = {}\n    layer_outputs = {}\n\n    \"\"\"\n    Create 2 hooks to capture the input and the output of the layers.\n    These hooks store the inputs and outputs in dictionaries\n    (layer_inputs and layer_outputs) for later analysis\n    \"\"\"\n    #Allows capture the input to the query projection (q_proj)\n    def q_proj_input_hook(layer_idx):\n        def _hook(module, module_input):\n            # module_input can be a tuple depending on PyTorch version\n            inp = module_input[0] if isinstance(module_input, tuple) else module_input\n            layer_inputs[layer_idx] = inp.detach().clone()\n        return _hook\n\n    # Allows capture the output from the output projection (o_proj)\n    def o_proj_output_hook(layer_idx):\n        def _hook(module, module_input, module_output):\n            out = module_output[0] if isinstance(module_output, tuple) else module_output\n            layer_outputs[layer_idx] = out.detach().clone()\n        return _hook\n\n    # Register hooks for each unpruned layer\n    handles = []\n    for idx in unpruned_layer_indices:\n        layer = pruned_model.model.layers[idx]\n        handles.append(layer.self_attn.q_proj.register_forward_pre_hook(q_proj_input_hook(idx)))\n        handles.append(layer.self_attn.o_proj.register_forward_hook(o_proj_output_hook(idx)))\n\n    # FORWARD PASS\n    \"\"\"\n    Single forward pass (no gradient needed)\n    A single forward pass is performed on the input text.\n    During this pass, the hooks capture the inputs and outputs of the unpruned layers.\n    This step is done with torch.no_grad(),\n    ensuring no gradients are calculated, which saves memory and computation.\n    \"\"\"\n    with torch.no_grad():\n        _ = pruned_model(**inputs)\n\n    \"\"\"\n    The hooks are removed after the forward pass\n    to avoid memory leaks or interference with subsequent operations.\n    \"\"\"\n    for h in handles:\n        h.remove()\n\n\n    #COMPUTE IMPORTANCE SCORES\n    \"\"\"\n    For each unpruned layer, the inputs and outputs are flattened into vectors for comparison.\n\n    Cosine Similarity: The similarity between the input and output vectors is\n    computed using cosine similarity. Layers with outputs that are very similar\n    to their inputs likely contribute less to the model’s overall computation.\n\n    Importance Score: The importance score for each layer is calculated as 1−similarity\n    A higher score indicates that the layer transforms its input significantly\n    and is therefore more important to the model's function.\n    \"\"\"\n    for idx in unpruned_layer_indices:\n        if idx in layer_inputs and idx in layer_outputs:\n            inp = layer_inputs[idx]\n            out = layer_outputs[idx]\n\n            inp_flat = inp.view(inp.size(0), -1)\n            out_flat = out.view(out.size(0), -1)\n\n            similarity = F.cosine_similarity(inp_flat, out_flat, dim=1).mean().item()\n            importance_score = 1 - similarity\n            importance_scores.append((idx, importance_score))\n\n            print(f\"[Iterative] Layer {idx} importance score: {importance_score:.4f}\")\n\n    \"\"\"A list of tuples is returned, where each tuple contains the layer index\n    and its calculated importance score.\"\"\"\n    return importance_scores","metadata":{"id":"x1hpx6MwBQAA","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:41.272067Z","iopub.execute_input":"2025-02-14T22:41:41.272341Z","iopub.status.idle":"2025-02-14T22:41:41.287022Z","shell.execute_reply.started":"2025-02-14T22:41:41.272319Z","shell.execute_reply":"2025-02-14T22:41:41.285935Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def bypass_single_layer(pruned_model, layer_idx):\n    \"\"\"\n    Modifies the specified layer's forward method so that attention is bypassed.\n    \"\"\"\n    layer = pruned_model.model.layers[layer_idx]\n    # Store the original forward.\n    if not hasattr(layer.self_attn, '_original_forward'):\n        layer.self_attn._original_forward = layer.self_attn.forward\n\n    # A new forward that checks whether to bypass\n    def new_attention_forward(self, hidden_states, attention_mask=None, position_ids=None,\n                              past_key_value=None, output_attentions=False, use_cache=False,\n                              **kwargs):\n        # If this layer is in drop_attn_list, bypass\n        if getattr(self, 'layer_idx', -1) in pruned_model.config.drop_attn_list:\n            return hidden_states, None, None\n        # Otherwise, use the original forward\n        return self._original_forward(hidden_states, attention_mask, position_ids,\n                                      past_key_value, output_attentions, use_cache, **kwargs)\n\n    # Set the layer index and forward\n    layer.self_attn.layer_idx = layer_idx\n    layer.self_attn.forward = new_attention_forward.__get__(layer.self_attn, type(layer.self_attn))\n","metadata":{"id":"vJAFpmLfBP-F","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:41.287923Z","iopub.execute_input":"2025-02-14T22:41:41.288189Z","iopub.status.idle":"2025-02-14T22:41:41.306349Z","shell.execute_reply.started":"2025-02-14T22:41:41.288168Z","shell.execute_reply":"2025-02-14T22:41:41.305341Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def iterative_pruning(model, tokenizer, input_text, num_layers_to_prune):\n    \"\"\"\n    Iteratively:\n      1) Measures importance of unpruned layers,\n      2) Prunes (bypasses) the least important layer,\n      3) Repeats until num_layers_to_prune layers are pruned.\n    \"\"\"\n    # Create a copy of the model so we don't modify the original\n    pruned_model = deepcopy(model)\n\n    # Make sure we have a list of pruned layers in config\n    pruned_model.config.drop_attn_list = []\n\n    total_layers = len(pruned_model.model.layers)\n    print(f\"Total layers: {total_layers}\")\n\n    for step in range(num_layers_to_prune):\n        print(f\"\\n--- Iteration {step + 1} of {num_layers_to_prune} ---\")\n\n        # 1) Measure importance scores for all unpruned layers\n        importance_scores = measure_unpruned_layer_importances(pruned_model, tokenizer, input_text)\n        if not importance_scores:\n            print(\"No unpruned layers found or no importance scores computed.\")\n            break\n\n        # 2) Pick layer with the lowest importance\n        layer_to_bypass, min_score = min(importance_scores, key=lambda x: x[1])\n\n        # 3) Bypass that layer\n        pruned_model.config.drop_attn_list.append(layer_to_bypass)\n        bypass_single_layer(pruned_model, layer_to_bypass)\n\n        print(f\"Bypassing layer {layer_to_bypass} with importance score {min_score:.4f}\")\n        print(f\"Current bypass list: {pruned_model.config.drop_attn_list}\")\n\n    print(f\"\\nFinal bypassed layers: {sorted(pruned_model.config.drop_attn_list)}\")\n    print(f\"Number of bypassed layers: {len(pruned_model.config.drop_attn_list)}\")\n\n    return pruned_model","metadata":{"id":"b5oymq28BP76","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:41.307351Z","iopub.execute_input":"2025-02-14T22:41:41.307749Z","iopub.status.idle":"2025-02-14T22:41:41.325966Z","shell.execute_reply.started":"2025-02-14T22:41:41.307715Z","shell.execute_reply":"2025-02-14T22:41:41.325197Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Prunning the model","metadata":{"id":"ewstgaKgDCE2"}},{"cell_type":"code","source":"pruned_model = iterative_pruning(\n      model,\n      tokenizer,\n       \"Hi I'm a sample text, use to calculate teh cosine difference between input and output.\",\n      num_layers_to_prune=4\n)","metadata":{"id":"XAAkKyVbBP5t","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:41.326660Z","iopub.execute_input":"2025-02-14T22:41:41.326975Z","iopub.status.idle":"2025-02-14T22:41:41.801451Z","shell.execute_reply.started":"2025-02-14T22:41:41.326935Z","shell.execute_reply":"2025-02-14T22:41:41.800669Z"}},"outputs":[{"name":"stdout","text":"Total layers: 28\n\n--- Iteration 1 of 4 ---\n[Iterative] Layer 0 importance score: 1.0713\n[Iterative] Layer 1 importance score: 0.9355\n[Iterative] Layer 2 importance score: 0.9594\n[Iterative] Layer 3 importance score: 0.9596\n[Iterative] Layer 4 importance score: 0.9597\n[Iterative] Layer 5 importance score: 0.9989\n[Iterative] Layer 6 importance score: 1.0272\n[Iterative] Layer 7 importance score: 1.0362\n[Iterative] Layer 8 importance score: 1.1125\n[Iterative] Layer 9 importance score: 1.1243\n[Iterative] Layer 10 importance score: 1.0488\n[Iterative] Layer 11 importance score: 0.9959\n[Iterative] Layer 12 importance score: 1.0859\n[Iterative] Layer 13 importance score: 1.0245\n[Iterative] Layer 14 importance score: 0.9680\n[Iterative] Layer 15 importance score: 0.9707\n[Iterative] Layer 16 importance score: 0.9034\n[Iterative] Layer 17 importance score: 0.9299\n[Iterative] Layer 18 importance score: 1.0231\n[Iterative] Layer 19 importance score: 0.7273\n[Iterative] Layer 20 importance score: 0.8678\n[Iterative] Layer 21 importance score: 0.9238\n[Iterative] Layer 22 importance score: 0.8560\n[Iterative] Layer 23 importance score: 0.7809\n[Iterative] Layer 24 importance score: 0.8403\n[Iterative] Layer 25 importance score: 0.8705\n[Iterative] Layer 26 importance score: 0.7748\n[Iterative] Layer 27 importance score: 0.9776\nBypassing layer 19 with importance score 0.7273\nCurrent bypass list: [19]\n\n--- Iteration 2 of 4 ---\n[Iterative] Layer 0 importance score: 1.0713\n[Iterative] Layer 1 importance score: 0.9355\n[Iterative] Layer 2 importance score: 0.9594\n[Iterative] Layer 3 importance score: 0.9596\n[Iterative] Layer 4 importance score: 0.9597\n[Iterative] Layer 5 importance score: 0.9989\n[Iterative] Layer 6 importance score: 1.0272\n[Iterative] Layer 7 importance score: 1.0362\n[Iterative] Layer 8 importance score: 1.1125\n[Iterative] Layer 9 importance score: 1.1243\n[Iterative] Layer 10 importance score: 1.0488\n[Iterative] Layer 11 importance score: 0.9959\n[Iterative] Layer 12 importance score: 1.0859\n[Iterative] Layer 13 importance score: 1.0245\n[Iterative] Layer 14 importance score: 0.9680\n[Iterative] Layer 15 importance score: 0.9707\n[Iterative] Layer 16 importance score: 0.9034\n[Iterative] Layer 17 importance score: 0.9299\n[Iterative] Layer 18 importance score: 1.0231\n[Iterative] Layer 20 importance score: 0.9103\n[Iterative] Layer 21 importance score: 0.9540\n[Iterative] Layer 22 importance score: 0.9023\n[Iterative] Layer 23 importance score: 0.8628\n[Iterative] Layer 24 importance score: 0.9123\n[Iterative] Layer 25 importance score: 0.9043\n[Iterative] Layer 26 importance score: 0.9029\n[Iterative] Layer 27 importance score: 1.2322\nBypassing layer 23 with importance score 0.8628\nCurrent bypass list: [19, 23]\n\n--- Iteration 3 of 4 ---\n[Iterative] Layer 0 importance score: 1.0713\n[Iterative] Layer 1 importance score: 0.9355\n[Iterative] Layer 2 importance score: 0.9594\n[Iterative] Layer 3 importance score: 0.9596\n[Iterative] Layer 4 importance score: 0.9597\n[Iterative] Layer 5 importance score: 0.9989\n[Iterative] Layer 6 importance score: 1.0272\n[Iterative] Layer 7 importance score: 1.0362\n[Iterative] Layer 8 importance score: 1.1125\n[Iterative] Layer 9 importance score: 1.1243\n[Iterative] Layer 10 importance score: 1.0488\n[Iterative] Layer 11 importance score: 0.9959\n[Iterative] Layer 12 importance score: 1.0859\n[Iterative] Layer 13 importance score: 1.0245\n[Iterative] Layer 14 importance score: 0.9680\n[Iterative] Layer 15 importance score: 0.9707\n[Iterative] Layer 16 importance score: 0.9034\n[Iterative] Layer 17 importance score: 0.9299\n[Iterative] Layer 18 importance score: 1.0231\n[Iterative] Layer 20 importance score: 0.9103\n[Iterative] Layer 21 importance score: 0.9540\n[Iterative] Layer 22 importance score: 0.9023\n[Iterative] Layer 24 importance score: 0.9365\n[Iterative] Layer 25 importance score: 0.9225\n[Iterative] Layer 26 importance score: 0.9329\n[Iterative] Layer 27 importance score: 1.3022\nBypassing layer 22 with importance score 0.9023\nCurrent bypass list: [19, 23, 22]\n\n--- Iteration 4 of 4 ---\n[Iterative] Layer 0 importance score: 1.0713\n[Iterative] Layer 1 importance score: 0.9355\n[Iterative] Layer 2 importance score: 0.9594\n[Iterative] Layer 3 importance score: 0.9596\n[Iterative] Layer 4 importance score: 0.9597\n[Iterative] Layer 5 importance score: 0.9989\n[Iterative] Layer 6 importance score: 1.0272\n[Iterative] Layer 7 importance score: 1.0362\n[Iterative] Layer 8 importance score: 1.1125\n[Iterative] Layer 9 importance score: 1.1243\n[Iterative] Layer 10 importance score: 1.0488\n[Iterative] Layer 11 importance score: 0.9959\n[Iterative] Layer 12 importance score: 1.0859\n[Iterative] Layer 13 importance score: 1.0245\n[Iterative] Layer 14 importance score: 0.9680\n[Iterative] Layer 15 importance score: 0.9707\n[Iterative] Layer 16 importance score: 0.9034\n[Iterative] Layer 17 importance score: 0.9299\n[Iterative] Layer 18 importance score: 1.0231\n[Iterative] Layer 20 importance score: 0.9103\n[Iterative] Layer 21 importance score: 0.9540\n[Iterative] Layer 24 importance score: 0.9606\n[Iterative] Layer 25 importance score: 0.9466\n[Iterative] Layer 26 importance score: 0.9412\n[Iterative] Layer 27 importance score: 1.3245\nBypassing layer 16 with importance score 0.9034\nCurrent bypass list: [19, 23, 22, 16]\n\nFinal bypassed layers: [16, 19, 22, 23]\nNumber of bypassed layers: 4\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Test the Model","metadata":{"id":"i0oUFks8DMR8"}},{"cell_type":"code","source":"import time\n\ndef get_output(prompt, model=model, tokenizer=tokenizer, num_runs=1, max_length=50):\n    total_time = 0\n    generated_outputs = []\n\n    for run in range(num_runs):\n        # Start timing\n        start_time = time.time()\n\n        # Tokenization time\n        token_start = time.time()\n        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n        token_time = time.time() - token_start\n\n        # Generation time\n        gen_start = time.time()\n        outputs = model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_length=max_length,\n            num_return_sequences=1,\n            pad_token_id=tokenizer.pad_token_id,\n            temperature=None,\n            top_p=None,\n            do_sample=False,  # Disable sampling\n            num_beams=5,      # Use beam search\n            early_stopping=True,  # Stop when end-of-sequence token is generated\n            no_repeat_ngram_size=2  # Prevent repetition of 2-grams\n        )\n        gen_time = time.time() - gen_start\n\n        # Decoding time\n        decode_start = time.time()\n        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        decode_time = time.time() - decode_start\n\n        # Total time for this run\n        total_time += time.time() - start_time\n        generated_outputs.append(generated)\n\n        # Measure memory usage\n        memory_allocated = torch.cuda.memory_allocated() / (1024 ** 2)  # In MB\n        memory_reserved = torch.cuda.memory_reserved() / (1024 ** 2)  # In MB\n\n        print(f\"Memory Allocated: {memory_allocated:.2f} MB\")\n        print(f\"Memory Reserved: {memory_reserved:.2f} MB\")\n\n        if num_runs > 1:\n            print(f\"\\nRun {run + 1}:\")\n        print(f\"Tokenization time: {token_time*1000:.2f} ms\")\n        print(f\"Generation time: {gen_time*1000:.2f} ms\")\n        print(f\"Decoding time: {decode_time*1000:.2f} ms\")\n        print(f\"Total time: {(time.time() - start_time)*1000:.2f} ms\")\n\n    if num_runs > 1:\n        avg_time = total_time / num_runs\n        print(f\"\\nAverage time over {num_runs} runs: {avg_time*1000:.2f} ms\")\n\n    return generated_outputs[0] if num_runs == 1 else generated_outputs","metadata":{"id":"1Cv_laCEBP3g","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:41.802227Z","iopub.execute_input":"2025-02-14T22:41:41.802446Z","iopub.status.idle":"2025-02-14T22:41:41.809613Z","shell.execute_reply.started":"2025-02-14T22:41:41.802427Z","shell.execute_reply":"2025-02-14T22:41:41.808797Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Test the original model\nprompt = \"Dhaka is the capital of\"\ngenerated = get_output(prompt, num_runs=2)\nprint(f\"Generated text: {generated}\")","metadata":{"id":"fBtFzhBlBP1J","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:41.810511Z","iopub.execute_input":"2025-02-14T22:41:41.810858Z","iopub.status.idle":"2025-02-14T22:41:48.085872Z","shell.execute_reply.started":"2025-02-14T22:41:41.810829Z","shell.execute_reply":"2025-02-14T22:41:48.084985Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Memory Allocated: 12264.82 MB\nMemory Reserved: 12398.00 MB\n\nRun 1:\nTokenization time: 0.44 ms\nGeneration time: 3160.33 ms\nDecoding time: 0.26 ms\nTotal time: 3162.14 ms\nMemory Allocated: 12264.82 MB\nMemory Reserved: 12398.00 MB\n\nRun 2:\nTokenization time: 0.64 ms\nGeneration time: 3093.70 ms\nDecoding time: 0.19 ms\nTotal time: 3095.05 ms\n\nAverage time over 2 runs: 3127.78 ms\nGenerated text: ['Dhaka is the capital of Bangladesh. It is located on the banks of the Buriganga River, which flows into the Bay of Bengal. The city has a population of over 10 million people, making it the largest city in Bangladesh', 'Dhaka is the capital of Bangladesh. It is located on the banks of the Buriganga River, which flows into the Bay of Bengal. The city has a population of over 10 million people, making it the largest city in Bangladesh']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Test the pruned model\ngenerated = get_output(prompt, pruned_model, num_runs=2)\nprint(f\"Generated text: {generated}\")","metadata":{"id":"5He_1dzzBPxj","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:48.086775Z","iopub.execute_input":"2025-02-14T22:41:48.087127Z","iopub.status.idle":"2025-02-14T22:41:54.058359Z","shell.execute_reply.started":"2025-02-14T22:41:48.087093Z","shell.execute_reply":"2025-02-14T22:41:54.057333Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Memory Allocated: 12264.82 MB\nMemory Reserved: 12398.00 MB\n\nRun 1:\nTokenization time: 1.00 ms\nGeneration time: 2980.12 ms\nDecoding time: 0.18 ms\nTotal time: 2981.82 ms\nMemory Allocated: 12264.82 MB\nMemory Reserved: 12398.00 MB\n\nRun 2:\nTokenization time: 1.18 ms\nGeneration time: 2981.45 ms\nDecoding time: 0.20 ms\nTotal time: 2983.37 ms\n\nAverage time over 2 runs: 2982.07 ms\nGenerated text: ['Dhaka is the capital of Bangladesh Bangladesh is a country located in Asia-Pacific region. It is also known as “””Bang Bang Bang bang bangbangbang-b-b-B B B b bB B-B-B- B.B.B-B', 'Dhaka is the capital of Bangladesh Bangladesh is a country located in Asia-Pacific region. It is also known as “””Bang Bang Bang bang bangbangbang-b-b-B B B b bB B-B-B- B.B.B-B']\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Storing the model\n","metadata":{"id":"0E7yNTN1ELap"}},{"cell_type":"code","source":"new_model_name = 'attnprun-llama-3.2-3B'\noutput_dir = './'+new_model_name\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\npruned_model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n#new_config.save_pretrained(output_dir)\nprint(f\"Pruned model saved to {output_dir}\")","metadata":{"id":"XxeNzPQPEPiz","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:41:54.059408Z","iopub.execute_input":"2025-02-14T22:41:54.059768Z","iopub.status.idle":"2025-02-14T22:42:12.360172Z","shell.execute_reply.started":"2025-02-14T22:41:54.059736Z","shell.execute_reply":"2025-02-14T22:42:12.359234Z"}},"outputs":[{"name":"stdout","text":"Pruned model saved to ./attnprun-llama-3.2-3B\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:43:15.938726Z","iopub.execute_input":"2025-02-14T22:43:15.939055Z","iopub.status.idle":"2025-02-14T22:43:15.955825Z","shell.execute_reply.started":"2025-02-14T22:43:15.939031Z","shell.execute_reply":"2025-02-14T22:43:15.955108Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16de693465aa4795b0a1a242b5857add"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Push the model to your Hugging Face repository\npruned_model.push_to_hub(new_model_name, private=False)\ntokenizer.push_to_hub(new_model_name)","metadata":{"id":"abF1jSn3EPW_","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:43:27.792146Z","iopub.execute_input":"2025-02-14T22:43:27.792477Z","iopub.status.idle":"2025-02-14T22:46:00.351154Z","shell.execute_reply.started":"2025-02-14T22:43:27.792454Z","shell.execute_reply":"2025-02-14T22:46:00.350425Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90932788ea644363878cc2be6fa7862f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117e70bbd0dc44348b50b0e51c322265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f2402c1477a483ca0a006009c2fe5f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22f788d9668040c991ac6f4357f15479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeaf9a625dfc40e68cb9d56413f827a0"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Shahrukh0/attnprun-llama-3.2-3B/commit/759df17f502e6f279e73d3ef4a2f3f4361975e55', commit_message='Upload tokenizer', commit_description='', oid='759df17f502e6f279e73d3ef4a2f3f4361975e55', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Shahrukh0/attnprun-llama-3.2-3B', endpoint='https://huggingface.co', repo_type='model', repo_id='Shahrukh0/attnprun-llama-3.2-3B'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}